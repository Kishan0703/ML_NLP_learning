{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ebc3a3",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97f3cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: \n",
      "Artificial Intelligence (AI) is transforming industries!\n",
      "From healthcare to finance, AI is helping automate processes, improve decision-making, and create innovative products.\n",
      "But, it also raises ethical concerns—such as job displacement and data privacy.\n",
      "Words:\n",
      "Artificial\n",
      "Intelligence\n",
      "(\n",
      "AI\n",
      ")\n",
      "is\n",
      "transforming\n",
      "industries\n",
      "!\n",
      "From\n",
      "healthcare\n",
      "to\n",
      "finance\n",
      ",\n",
      "AI\n",
      "is\n",
      "helping\n",
      "automate\n",
      "processes\n",
      ",\n",
      "improve\n",
      "decision-making\n",
      ",\n",
      "and\n",
      "create\n",
      "innovative\n",
      "products\n",
      ".\n",
      "But\n",
      ",\n",
      "it\n",
      "also\n",
      "raises\n",
      "ethical\n",
      "concerns—such\n",
      "as\n",
      "job\n",
      "displacement\n",
      "and\n",
      "data\n",
      "privacy\n",
      ".\n",
      "Total num of words is 36\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from string import punctuation\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from nltk import sent_tokenize,word_tokenize,wordpunct_tokenize\n",
    "\n",
    "corpus=\"Artificial Intelligence (AI) is transforming industries! From healthcare to finance, AI is helping automate processes, improve decision-making, and create innovative products. But, it also raises ethical concerns—such as job displacement and data privacy.\"\n",
    "\n",
    "sent=sent_tokenize(corpus)\n",
    "words=word_tokenize(corpus)\n",
    "wordswithoutpun=wordpunct_tokenize(corpus)\n",
    "\n",
    "print(\"Sentences: \")\n",
    "for i in sent:\n",
    "    print(i)\n",
    "\n",
    "print(\"Words:\")\n",
    "for i in words:\n",
    "    print(i)\n",
    "\n",
    "total=0\n",
    "for i in wordswithoutpun:\n",
    "    if i not in punctuation:\n",
    "        total+=1\n",
    "\n",
    "print(f\"Total num of words is {total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042fe579",
   "metadata": {},
   "source": [
    "## Stemming + Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54d43b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word ----> porter ----> regex ----> snowball\n",
      "children ----> children ----> children ----> children\n",
      "were ----> were ----> wer ----> were\n",
      "happily ----> happili ----> happily ----> happili\n",
      "playing ----> play ----> play ----> play\n",
      "playgrounds ----> playground ----> playground ----> playground\n",
      "while ----> while ----> whil ----> while\n",
      "wolves ----> wolv ----> wolve ----> wolv\n",
      "were ----> were ----> wer ----> were\n",
      "hunting ----> hunt ----> hunt ----> hunt\n",
      "nearby ----> nearbi ----> nearby ----> nearbi\n",
      "Studies ----> studi ----> Studie ----> studi\n",
      "about ----> about ----> about ----> about\n",
      "modernization ----> modern ----> modernization ----> modern\n",
      "globalizations ----> global ----> globalization ----> global\n",
      "digitalizations ----> digit ----> digitalization ----> digit\n",
      "rapidly ----> rapidli ----> rapidly ----> rapid\n",
      "increasing ----> increas ----> increas ----> increas\n",
      "However ----> howev ----> However ----> howev\n",
      "some ----> some ----> som ----> some\n",
      "organizations ----> organ ----> organization ----> organ\n",
      "disabling ----> disabl ----> disabl ----> disabl\n",
      "unnecessary ----> unnecessari ----> unnecessary ----> unnecessari\n",
      "features ----> featur ----> feature ----> featur\n",
      "simplify ----> simplifi ----> simplify ----> simplifi\n",
      "their ----> their ----> their ----> their\n",
      "systems ----> system ----> system ----> system\n",
      "['were', 'were', 'wer', 'were']\n",
      "['happily', 'happili', 'happily', 'happili']\n",
      "['while', 'while', 'whil', 'while']\n",
      "['wolves', 'wolv', 'wolve', 'wolv']\n",
      "['were', 'were', 'wer', 'were']\n",
      "['nearby', 'nearbi', 'nearby', 'nearbi']\n",
      "['Studies', 'studi', 'Studie', 'studi']\n",
      "['modernization', 'modern', 'modernization', 'modern']\n",
      "['globalizations', 'global', 'globalization', 'global']\n",
      "['digitalizations', 'digit', 'digitalization', 'digit']\n",
      "['rapidly', 'rapidli', 'rapidly', 'rapid']\n",
      "['However', 'howev', 'However', 'howev']\n",
      "['some', 'some', 'som', 'some']\n",
      "['organizations', 'organ', 'organization', 'organ']\n",
      "['unnecessary', 'unnecessari', 'unnecessary', 'unnecessari']\n",
      "['features', 'featur', 'feature', 'featur']\n",
      "['simplify', 'simplifi', 'simplify', 'simplifi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer,RegexpStemmer,SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "text=\"\"\"The children were happily playing in the playgrounds while the wolves were hunting nearby.  \n",
    "Studies about modernization, globalizations, and digitalizations are rapidly increasing.  \n",
    "However, some organizations are disabling unnecessary features to simplify their systems.\"\"\"\n",
    "\n",
    "tokens=word_tokenize(text)\n",
    "\n",
    "\n",
    "porter=PorterStemmer()\n",
    "regex=RegexpStemmer('ing$|s$|e$|able$', min=3)\n",
    "snowball=SnowballStemmer('english')\n",
    "lst=[]\n",
    "print(\"word ----> porter ----> regex ----> snowball\")\n",
    "for i in tokens:\n",
    "    if i not in punctuation and len(i)>=4:\n",
    "        print(f\"{i} ----> {porter.stem(i)} ----> {regex.stem(i)} ----> {snowball.stem(i)}\")\n",
    "        lst.append([i,porter.stem(i),regex.stem(i),snowball.stem(i)])\n",
    "\n",
    "for i in lst:\n",
    "    if i[1]!=i[2] or i[1]!=i[3] or i[2]!=i[3]:\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd246d2",
   "metadata": {},
   "source": [
    "## Lemitisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "Sent=\"\"\"The cats were chasing the mice while the children were running quickly towards the playgrounds.  \n",
    "She has been studying the effects of modernizations on societies.  \n",
    "Many people are happier when they are given responsibilities and recognition.\"\"\"\n",
    "words2=word_tokenize(Sent)\n",
    "\n",
    "for i in words2:\n",
    "    print(lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b39f3d",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480afadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text=\"Machine learning is a subset of artificial intelligence that enables systems to learn from data without being explicitly programmed.\"\n",
    "def rem_stopwords(text):\n",
    "    words=word_tokenize(text)\n",
    "    lst=[]\n",
    "    stop_words=stopwords.words('english')\n",
    "    for i in words:\n",
    "        if i.lower() not in stop_words and i not in punctuation:\n",
    "            lst.append(i)\n",
    "    return lst\n",
    "\n",
    "rem_stopwords(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f0a887",
   "metadata": {},
   "source": [
    "## Parts of Speech tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599a7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from string import punctuation\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "text=\"John is playing football in the park while his friends are studying at home.\"\n",
    "lst=[]\n",
    "nountag=[\"NN\",'NNS' ,\"NNP\",\"NNPS\"]\n",
    "verbtag=[\"VB\" ,'VBD','VBG' ,'VBN' ,'VBP' ,'VBZ']\n",
    "noun=[]\n",
    "verb=[]\n",
    "words=word_tokenize(text)\n",
    "stop_words=stopwords.words('english')\n",
    "\n",
    "for i in words:\n",
    "    if i not in stop_words and i not in punctuation:\n",
    "        if i not in lst:\n",
    "            lst.append(i)\n",
    "\n",
    "aftlst=pos_tag(lst,tagset=None, lang=\"eng\")\n",
    "print(f\"{'word':<15} pos\")\n",
    "for i in aftlst:\n",
    "    print(f\"{i[0]:<15} {i[1]}\")\n",
    "    if i[1] in nountag:\n",
    "        noun.append(i[0])\n",
    "    elif i[1] in verbtag :\n",
    "        verb.append(i[0])\n",
    "\n",
    "print(f\"Noun: {noun}\")\n",
    "print(f\"verb= {verb}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b1b32d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /Users/kishan/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/kishan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persons: ['Barack', 'Obama']\n",
      "Others : [('Hawaii', 'GPE'), ('United States', 'GPE'), ('Paris', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "text=\"\"\"Barack Obama was born in Hawaii. He was elected President of the United States in 2008. \n",
    "Later, he gave a speech in Paris about climate change.\"\"\"\n",
    "persons=[]\n",
    "\n",
    "others=[]\n",
    "lst=word_tokenize(text)\n",
    "tagged=pos_tag(lst,lang='eng')\n",
    "\n",
    "chunked=ne_chunk(tagged)\n",
    "\n",
    "# go throug this once again\n",
    "\n",
    "for subtree in chunked:\n",
    "    if hasattr(subtree, 'label'):   # Named Entity found\n",
    "        label = subtree.label()\n",
    "        name = \" \".join([token for token, pos in subtree.leaves()])\n",
    "        \n",
    "        if label == \"PERSON\":\n",
    "            persons.append(name)\n",
    "        else:\n",
    "            others.append((name, label))  # keep label too (like GPE, ORGANIZATION, etc.)\n",
    "\n",
    "print(\"Persons:\", persons)\n",
    "print(\"Others :\", others)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4325f6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
