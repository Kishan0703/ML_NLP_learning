{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ffc5035",
   "metadata": {},
   "source": [
    "## Spam Classification using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cba106",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"ham\", \"Hey, are we still meeting for dinner tonight?\"),\n",
    "    (\"spam\", \"Win $1000 cash instantly. Limited offer!\"),\n",
    "    (\"ham\", \"Can you send me the report by tomorrow morning?\"),\n",
    "    (\"spam\", \"Congratulations! You won a free iPhone. Click here now.\"),\n",
    "    (\"ham\", \"Happy birthday! Wishing you a great year ahead.\"),\n",
    "    (\"spam\", \"Get cheap loans approved in 24 hours. Apply today.\"),\n",
    "    (\"ham\", \"Let’s catch up over coffee this weekend.\"),\n",
    "    (\"spam\", \"Claim your free vacation package now.\"),\n",
    "    (\"ham\", \"Your order has been shipped and will arrive soon.\"),\n",
    "    (\"spam\", \"Hot singles in your area waiting to chat.\"),\n",
    "    (\"ham\", \"Don’t forget about the meeting at 10am.\"),\n",
    "    (\"spam\", \"Cheap medicines delivered to your door.\"),\n",
    "    (\"ham\", \"Thank you for your help yesterday.\"),\n",
    "    (\"spam\", \"Exclusive deal: Buy 1 get 2 free. Limited stock.\"),\n",
    "    (\"ham\", \"Do you want to join the gym with me?\"),\n",
    "    (\"spam\", \"Special discount on luxury watches. Hurry!\"),\n",
    "    (\"ham\", \"Movie night tonight? I’ll bring snacks.\"),\n",
    "    (\"spam\", \"Join now and get rich quickly.\"),\n",
    "    (\"ham\", \"Meeting got rescheduled to Monday.\"),\n",
    "    (\"spam\", \"Earn money from home. No experience needed!\"),\n",
    "    (\"ham\", \"Congrats on your promotion!\"),\n",
    "    (\"spam\", \"Your account has won a lucky draw. Verify details.\"),\n",
    "    (\"ham\", \"Let’s plan a trip during the holidays.\"),\n",
    "    (\"spam\", \"Click here to increase your followers instantly.\"),\n",
    "    (\"ham\", \"Can you call me back when you’re free?\"),\n",
    "    (\"spam\", \"Investment opportunity with guaranteed returns.\"),\n",
    "    (\"ham\", \"Dinner was amazing, thanks for inviting me.\"),\n",
    "    (\"spam\", \"Free trial for premium subscription.\"),\n",
    "    (\"ham\", \"Are you free for a quick call?\"),\n",
    "    (\"spam\", \"Winner! Call now to claim your reward.\"),\n",
    "    (\"ham\", \"Looking forward to our weekend trip.\"),\n",
    "    (\"spam\", \"Don’t miss this chance to double your income.\"),\n",
    "    (\"ham\", \"Your appointment is confirmed for tomorrow.\"),\n",
    "    (\"spam\", \"Lowest insurance rates available. Act fast!\"),\n",
    "    (\"ham\", \"Did you finish the assignment?\"),\n",
    "    (\"spam\", \"Congratulations, your number was chosen.\"),\n",
    "    (\"ham\", \"Good morning, have a nice day!\"),\n",
    "    (\"spam\", \"Get Viagra at half price. Order online.\"),\n",
    "    (\"ham\", \"The parcel has been delivered.\"),\n",
    "    (\"spam\", \"Unlock exclusive bonus offers today!\"),\n",
    "    (\"ham\", \"Please update me once the task is done.\"),\n",
    "    (\"spam\", \"Get instant credit card approval. No checks.\"),\n",
    "    (\"ham\", \"How was your vacation?\"),\n",
    "    (\"spam\", \"Free membership for the first 100 users.\"),\n",
    "    (\"ham\", \"Can you bring some snacks to the party?\"),\n",
    "    (\"spam\", \"You won a lottery worth $10,000.\"),\n",
    "    (\"ham\", \"See you at the conference tomorrow.\"),\n",
    "    (\"spam\", \"Earn $500 daily with this simple trick.\"),\n",
    "    (\"ham\", \"I’ll email you the notes after class.\"),\n",
    "    (\"spam\", \"Act now! Limited-time financial opportunity.\")\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00334a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey  still meet dinner tonight ', 'win       cash instant  limit offer ', 'send report tomorrow morn ', 'congratul  free iphon  click ', 'happi birthday  wish great year ahead ', 'get cheap loan approv    hour  appli today ', 'let catch coffe weekend ', 'claim free vacat packag ', 'order ship arriv soon ', 'hot singl area wait chat ', 'forget meet   ', 'cheap medicin deliv door ', 'thank help yesterday ', 'exclus deal  buy   get   free  limit stock ', 'want join gym ', 'special discount luxuri watch  hurri ', 'movi night tonight  bring snack ', 'join get rich quick ', 'meet got reschedul monday ', 'earn money home  experi need ', 'congrat promot ', 'account lucki draw  verifi detail ', 'let plan trip holiday ', 'click increas follow instant ', 'call back free ', 'invest opportun guarante return ', 'dinner amaz  thank invit ', 'free trial premium subscript ', 'free quick call ', 'winner  call claim reward ', 'look forward weekend trip ', 'miss chanc doubl incom ', 'appoint confirm tomorrow ', 'lowest insur rate avail  act fast ', 'finish assign ', 'congratul  number chosen ', 'good morn  nice day ', 'get viagra half price  order onlin ', 'parcel deliv ', 'unlock exclus bonus offer today ', 'pleas updat task done ', 'get instant credit card approv  check ', 'vacat ', 'free membership first     user ', 'bring snack parti ', 'lotteri worth         ', 'see confer tomorrow ', 'earn      daili simpl trick ', 'email note class ', 'act  limit time financi opportun ']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "sp=SnowballStemmer('english')\n",
    "corpus=[]\n",
    "for i in range(0,len(messages)):\n",
    "    modisent=re.sub('[^a-zA-Z]',' ',messages[i][1])\n",
    "    modisent=modisent.lower()\n",
    "    modisent=modisent.split(' ')\n",
    "    modisent=[ sp.stem(word) for word in modisent if word not in stopwords.words('english')]\n",
    "    modisent=' '.join(modisent)\n",
    "    corpus.append(modisent)\n",
    "\n",
    "print(corpus)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv= CountVectorizer(max_df=100,binary=True)\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad3b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (50, 200)\n",
      "Some features: ['account' 'act' 'approv' 'area' 'area wait' 'bring' 'bring snack' 'call'\n",
      " 'catch' 'catch coffe' 'chanc' 'chanc doubl' 'chat' 'cheap' 'cheap loan'\n",
      " 'cheap medicin' 'check' 'chosen' 'claim' 'claim free' 'claim reward'\n",
      " 'class' 'click' 'click increas' 'coffe' 'coffe weekend' 'confer'\n",
      " 'confer tomorrow' 'confirm' 'confirm tomorrow']\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kishan/code/genai/ml/workbook/ML_learning/venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize stemmer and stopwords\n",
    "sb = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "# Custom tokenizer: clean, split, remove stopwords, stem\n",
    "def stemmed_tokenizer(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)          # keep only letters\n",
    "    tokens = text.lower().split()                  # split by whitespace\n",
    "    tokens = [sb.stem(w) for w in tokens if w not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# CountVectorizer with custom tokenizer + n-grams\n",
    "cv = CountVectorizer(\n",
    "    tokenizer=stemmed_tokenizer,  # our custom stemmer-based tokenizer\n",
    "    max_features=200,             # top 200 features (increase if needed)\n",
    "    binary=True,                  # presence/absence (1/0)\n",
    "    ngram_range=(1, 2)            # use unigrams + bigrams\n",
    ")\n",
    "\n",
    "# messages = [(label, text), ...]\n",
    "X = cv.fit_transform([msg[1] for msg in messages])   # feature matrix\n",
    "y = [msg[0] for msg in messages]                     # labels (ham/spam)\n",
    "\n",
    "# Inspect\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Some features:\", cv.get_feature_names_out()[:30])  # first 30 features\n",
    "print(X.toarray()[:5])  # first 5 messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca6c6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1dc424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
